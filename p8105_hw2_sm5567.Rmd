---
title: "Homework 2"
author: "Shina Min"
date: "10_04_2023"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE)
```

### Problem 0

This solution focuses on a reproducible report containing code and text necessary for Problems 1-3, and is organized as an R Project. This was not prepared as a GitHub repo; examples for repository structure and git commits should be familiar from other elements of the course.

Throughout, we use appropriate text to describe our code and results, and use clear styling to ensure code is readable. 

```{r load_libraries}
library(tidyverse)
library(readxl)
```


### Problem 1

We clean the 538 `pols` data, which provides information on the number of national politicians who are democratic or republican at any given time. There are some values for which `prez_gop` is `2` -- these are months in which Ford became President following Nixon's resignation. In the new `president` variable created as part of our data cleaning, we code these as `gop` (same as values when `prez_gop` is `1`).

```{r clean_538_pols}
month_df = 
  tibble(
    month_num = 1:12,
    month_abb = month.abb,
    month = month.name
  )

pols =
  read_csv("./fivethirtyeight_datasets/pols-month.csv") |>
  separate(mon, into = c("year", "month_num", "day"), convert = TRUE) |>
  mutate(
    president = recode(prez_gop, "0" = "dem", "1" = "gop", "2" = "gop")) |>
  left_join(x = _, y = month_df) |> 
  select(year, month, everything(), -day, -starts_with("prez"))
```

We also clean the 538 `snp` data, which contains information related to Standard & Poorâ€™s stock market index.

```{r clean_538_snp}
snp = 
  read_csv(
    "./fivethirtyeight_datasets/snp.csv",
    col_types = cols(date = col_date(format = "%m/%d/%y"))) |>
  separate(date, into = c("year", "month_num", "day"), convert = TRUE) |>
  mutate(
    year = if_else(year > 2023, year - 100, year)) |> 
  left_join(x = _, y = month_df) |> 
  select(year, month, close) 
```

Finally, we tidy the `unemployment` data so that it can be merged with the `pols` and `snp` datasets.

```{r clean_538_unemp}
unemployment = 
  read_csv("./fivethirtyeight_datasets/unemployment.csv") |>
  rename(year = Year) |>
  pivot_longer(
    Jan:Dec, 
    names_to = "month_abb",
    values_to = "unemployment"
  ) |> 
  left_join(x = _, y = month_df) |> 
  select(year, month, unemployment)
```

Now we merge the three datasets!

```{r merge_538}
data_538 = 
  left_join(pols, snp) |>
  left_join(x = _, y = unemployment)

str(data_538)
```

Notice that there are some `NA` values in the `close` and `unemployment` variables, which indicate that the value of these variables is missing at those locations.

Let's talk about the 538 datasets. The `pols` data has `r nrow(pols)` observations and `r ncol(pols)` variables and tells us about the party affiliation distribution (democrat or republican) for governors and senators for a given year from years `r pols |> pull(year) |> min()` to `r pols |> pull(year) |> max()`. It also tells us whether the sitting president was a democrat or republican. The `snp` data has `r nrow(snp)` observations and `r ncol(snp)` variables, ranging from years `r snp |> pull(year) |> min()` to `r snp |> pull(year) |> max()`. The `unemployment` data has `r nrow(unemployment)` observations and `r ncol(unemployment)` variables ranging from years `r unemployment |> pull(year) |> min()` to `r unemployment |> pull(year) |> max()`. In Januarys in or after 1975 in which a democrat was president, the **average unemployment rate was `r filter(data_538, month == "January", year >= 1975, president == "dem") |> pull(unemployment) |> mean() |> round(2)`**.  The average unemployment rate over the same time period in which a republican was president was `r filter(data_538, month == "January", year >= 1975, president == "gop") |> pull(unemployment) |> mean() |> round(2)`.


### Problem 2

This problem uses the Mr. Trash Wheel dataset, available as an Excel file on the course website.


```{r}
library(tidyverse)
library(readxl)
library(dplyr)
```

## Reading and cleaning Trash Wheel Collection Data

```{r}
mr_trash_wheel =
  read_excel("./202309 Trash Wheel Collection Data.xlsx",
             sheet = "Mr. Trash Wheel",
             range = cell_cols("A:N")) %>%
  janitor::clean_names() %>%
  drop_na(dumpster) %>%
  mutate(homes_powered = (weight_tons * 500) / 30,
         trash_wheel_data = 1,
         year = as.numeric(year))

prof_trash_wheel =
  read_excel("./202309 Trash Wheel Collection Data.xlsx",
             sheet = "Professor Trash Wheel",
             range = cell_cols("A:M")) %>%
  janitor::clean_names() %>%
  drop_na(dumpster) %>%
  mutate(homes_powered = (weight_tons * 500) / 30,
         trash_wheel_data = 2)

gwy_trash_wheel =
  read_excel("./202309 Trash Wheel Collection Data.xlsx",
             sheet = "Gwynnda Trash Wheel",
             range = cell_cols("A:J")) %>%
  janitor::clean_names() %>% 
  drop_na(dumpster) %>%
  mutate(homes_powered = (weight_tons * 500) / 30,
         trash_wheel_data = 3)
```

* To load the `mr_trash_wheel` dataset, I used the `read_excel` function and specified the sheet and range by using `sheet` and `range` functions. Then I cleaned the name using the `janitor::clean_names()` function. I dropped the dumpster data (unnecessary data that doesn't incoude dumpster-specific data). I used calculation described in the Homes powered note and updated the data to include a new homes_powered variable based on this calculation by using the `mutate()` function, multiplying `weight_tons` by 500 and divded them with 30. Lastly, I converted the year variable to a numeric data tye by using the `as.numeric()`.
* I repeated the same step for the rest of `prof_trash_wheel` dataset and `gwy_trash_wheel` dataset.

## Merging three data sets

```{r}
trash_wheel_data =
 bind_rows(mr_trash_wheel, prof_trash_wheel, gwy_trash_wheel) %>% ## merging datasets using bind_row function
  janitor::clean_names() ## cleaning the names, using the janitor function
trash_wheel_data ## loading the merged datasets
```

* In the `r`mr_trash_wheel` dataset, there are 15 variables, and 584 observations. The average number of homes powered is 53.3 and the total weight of trash collected is 1875 tons.

* The `r`prof_trash_wheel` dataset includes 14 variables and 106 observations. The average number of homes powered is 34. The total weight of trash collected is 216 tons.

* Lastly, the `r`gwy_trash_wheel` dataset has 12 variables and 155 observations. The average number of homes powered is 48.6 and the total weight of trash collected is 452 tons. The total number of cigarette butts collected by Gwynnda in July of 2021 is 16300.


### Problem 3

## Importing, cleaning and tidying the dataset + mutating status + removing participants do not meet the criteria.

```{r}
mci_df =
  read_csv("data_mci/MCI_baseline.csv", skip = 1, na = ".") %>% ## reading mci_df in csv
  janitor::clean_names() %>% ## cleaning the names, using the janitor function
  mutate(sex = recode(sex, `0` = "male", `1` = "female")) %>%
  mutate(apoe4 = recode(apoe4, "1" = "carrier", "0" = "non-carrier")) %>%
  filter(age_at_onset != ".", age_at_onset > current_age)
## selecting some variables and then mutating and then getting rid of the missing values.

female_apoe4 =
  filter(mci_df, sex == "female")
nrow(female_apoe4) ## the proportion of women who are APOE4 carriers using nrow function
```

* To import, clean and tidy the dataset of baseline demographics, I started giving the data set a name. I defined the baseline data as `r`mci_df()`.
* I had to remove the first row of data due to it containing repeat information that is already found in the columns.
* Then, I read mci_df in csv, using `r `read_csv` function I learned in lectures.
* Next, I cleaned the names by using `r`janitor::clean_names()` function.
* Since sex data is in numeric values (male = 0, female =1), I changed the values to male and female by using `r `mutate()` and `r `recode()`function as I learned in class.
* I also changed APOE4 carieer status from numeric values to carrier and non-carrier. In order to change the APOE4 status, I also used `r `mutate()` function and `r `recode()` function.
* Lastly, to eliminate any participants who do not meet the stated inclusion criteria (with no MCI at baseline), I used `r `filter()` function by putting `!` sign in front of the `.` data (no MCI at baseline) and kept all other columns by specifying the condition using `r `age_at_onset > current_age`.
* `mci_df` has 93 observations and 6 variables whereas, `female_apoe4` has 48 observations of 6 variables. Key variables includ `id`, `current_age`, `sex`, `education`, `apoe4`, and `age_at_onset`. 


## Number of people developed MCI, the average baseline age and the proportion of APOE4 carrier women.

```{r}
mean(mci_df$current_age) ## finding the mean age of baseline age

nrow(mci_df)
ncol(mci_df)
filter(mci_df, age_at_onset !=".") %>%
  nrow()
```

* Number of participants were reurited is 479 (from the datasets), 93 people develop MCI during the study. In the study contained `r nrow(mci_df)`, participants and had 'r ncol(mci_df)` variables. of the `r nrow(mci_df)` participants recruited at baseline, `r filter(mci_df, age_at_onset !=".") |> nrow()` developed MCI during the study.
* The mean age of baseline age is 65.542. I used `r mean(mci_df$current_age)` function to find the mean age of baseline age.
* The proportion of women in the study that are also APOE4 carriers is 48 %. To find this proportion, I used `r female_apoe4 = filter(mci_df, sex == "female")` and then `r nrow(female_apoe4)` and got 48.

## Importing, cleaning, tidying and combining the dataset of longitudinally observed biomarker values.

```{r}
amyloid_df =
  read.csv("data_mci/mci_amyloid.csv", skip =1) %>% ## reading amyloid_df in read_csv
  janitor::clean_names() ## cleaning names, using janitor::clean_names function

colnames(amyloid_df)[colnames(amyloid_df) == "study_id"] = "id"
## Specifying what are the column names by using colnames function

all_data =
  inner_join(mci_df, amyloid_df, by = "id")
all_data ## combining all the datasets by using inner_join function

```

* I had to remove the first row of data due to it containing repeat information that is already found in the columns. 
* Then I read the `amyloid_df` in csv by using the `read.csv` function. Next, I cleaned the name by using the `janitor::clean_names()` function.
* To specify the column names, I used `colnames(amyloid_df)` and renamed `student_id` to "id" for consistency when merging.
* The dataset contains participant information during the study's follow-up period.
* `amyloid_df` has 487 observations of 6 variables. Key variables include `id` and `baseline`, as well as fixed time intervals labeled `time_2`, `time_4`, `time_6`, and `time_8`.
* Lastly, I merged all the datasets of `mci_df` and `amyloid_df` by using the `inner_join()` function.
* I printed out the dataset, putting the `all_data` function.
* `all_data` has 90 observations and 11 variables including current_age, sex, education, apoe4, age_at_onset, baseline, time2, time4, time6, and time8.



## Participants appear in only the amyloid datasets:

All participants in `mci_df` (baseline) are in `amyloid_df`. However, participants with study id's from 472 to 495 are only in the `amyloid_df`. Without specific information, I could assume that some participants may only present in the `amyloid_df` and not the `mci_df` (baseline) because of participants being added to the dataset after the baseline data collection started. 


## Exporting the result as a CSV to my data directory. 
 
```{r}
write.csv(all_data, "data_mci/baseline_amyloid_merged.csv")
```

* I exported the `all_data` (combined data), using `write.csv` function and stored in my `data_mci` folder as "baseline_amyloid_merged.csv"